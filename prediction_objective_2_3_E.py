# -*- coding: utf-8 -*-
"""2_3_E

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ggFIwLA9wLTvydqWWVYsSsU8yevEWVCv

# **PROYECTO: PREDICCI√ìN DE PRECIOS BAJOS EN BOLETOS DE AVI√ìN EN VUELOS NACIONALES DE PER√ö EN LA AEROL√çNE LATAM**

------
"""

import pandas as pd
import json

class ResultadosModelos:
    def __init__(self, modelos):
        self.objetivo2_cols = ['MAE', 'MSE', 'RMSE', 'R2', 'Tiempo']
        self.objetivo3_cols = ['accuracy', 'precision', 'recall', 'f1', 'Tiempo']

        self.columns = pd.MultiIndex.from_tuples(
            [('OBJETIVO 2', col) for col in self.objetivo2_cols] +
            [('OBJETIVO 3', col) for col in self.objetivo3_cols] +
            [('HIPERPAR√ÅMETROS OBJ2', '')] +
            [('HIPERPAR√ÅMETROS OBJ3', '')]
        )

        self.modelos = modelos
        self.df_resultados = pd.DataFrame(index=self.modelos, columns=self.columns)

    def agregar_resultados_objetivo2(self, modelo, mae, mse, rmse, r2, tiempo):
        self.df_resultados.loc[modelo, ('OBJETIVO 2', 'MAE')] = mae
        self.df_resultados.loc[modelo, ('OBJETIVO 2', 'MSE')] = mse
        self.df_resultados.loc[modelo, ('OBJETIVO 2', 'RMSE')] = rmse
        self.df_resultados.loc[modelo, ('OBJETIVO 2', 'R2')] = r2
        self.df_resultados.loc[modelo, ('OBJETIVO 2', 'Tiempo')] = tiempo

    def agregar_resultados_objetivo3(self, modelo, accuracy, precision, recall, f1, tiempo):
        self.df_resultados.loc[modelo, ('OBJETIVO 3', 'accuracy')] = accuracy
        self.df_resultados.loc[modelo, ('OBJETIVO 3', 'precision')] = precision
        self.df_resultados.loc[modelo, ('OBJETIVO 3', 'recall')] = recall
        self.df_resultados.loc[modelo, ('OBJETIVO 3', 'f1')] = f1
        self.df_resultados.loc[modelo, ('OBJETIVO 3', 'Tiempo')] = tiempo

    def agregar_hiperparametros_obj2(self, modelo, hiperparametros_dict):
        self.df_resultados.loc[modelo, ('HIPERPAR√ÅMETROS OBJ2', '')] = json.dumps(hiperparametros_dict, ensure_ascii=False)

    def agregar_hiperparametros_obj3(self, modelo, hiperparametros_dict):
        self.df_resultados.loc[modelo, ('HIPERPAR√ÅMETROS OBJ3', '')] = json.dumps(hiperparametros_dict, ensure_ascii=False)

    def guardar_csv(self, nombre_archivo='resultados_modelos.csv'):
        self.df_resultados.to_csv(nombre_archivo)
        print(f"‚úÖ Archivo guardado: {nombre_archivo}")

    def mostrar_resultados(self):
        print(self.df_resultados)

# Importar las librer√≠as necesarias
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Cargar el dataset
df = pd.read_csv('flights_data_final.csv')

# Mostrar las primeras filas
print("Primeras 5 filas del dataset:")
display(df.head())

# Mostrar informaci√≥n b√°sica del dataset
print("\nInformaci√≥n del dataset:")
print(df.info())

# Verificar los tipos de datos
print("\nTipos de datos iniciales:")
print(df.dtypes)

# Mostrar estad√≠sticas descriptivas de todas las columnas (sin filtrar por tipo)
print("\nEstad√≠sticas descriptivas de todas las columnas:")
print(df.describe(include='all'))

modelos = ['LSTM', 'GRU', 'TCN', 'TRANSFORMERS', 'Random Forest', 'Arbol de Decisi√≥n', 'KNN', 'XGBoost']
resultados = ResultadosModelos(modelos)

"""# **ENTRENAMIENTO 2: OBJETIVO PREDECIR D√çAS HASTA EL VUELO**

## **E2 - MODELO 1: LSTM**
"""

import optuna
import time
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Concatenate, Dropout, BatchNormalization, Reshape, Flatten
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# --- Divisi√≥n de datos
X = df[['Origen_int', 'Destino_int', 'Duracion_minutos', 'Directo_binario',
        'Tarifa', 'Rango_Horario_Consulta_int', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Dias_hasta_vuelo']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos', 'Tarifa']] = scaler.fit_transform(X_train[['Duracion_minutos', 'Tarifa']])
X_val[['Duracion_minutos', 'Tarifa']] = scaler.transform(X_val[['Duracion_minutos', 'Tarifa']])

# --- Optuna objective
def objective(trial):
    embedding_dim = trial.suggest_int('embedding_dim', 4, 16)
    lstm_units = trial.suggest_int('lstm_units', 32, 128)
    dense_units = trial.suggest_int('dense_units', 32, 128)
    dropout_rate = trial.suggest_float('dropout', 0.1, 0.5)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)

    # Entradas categ√≥ricas
    input_origen = Input(shape=(1,))
    input_destino = Input(shape=(1,))
    input_rango_horario = Input(shape=(1,))
    input_salida = Input(shape=(1,))
    input_llegada = Input(shape=(1,))
    input_features = Input(shape=(3,))

    embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
    embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
    embedding_rango_horario = Embedding(input_dim=X['Rango_Horario_Consulta_int'].nunique()+1, output_dim=embedding_dim)(input_rango_horario)
    embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
    embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

    x = Concatenate()([
        Flatten()(embedding_origen),
        Flatten()(embedding_destino),
        Flatten()(embedding_rango_horario),
        Flatten()(embedding_salida),
        Flatten()(embedding_llegada),
        input_features
    ])

    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)
    x = Reshape((1, x.shape[-1]))(x)
    x = LSTM(lstm_units)(x)
    x = Dense(dense_units, activation='relu')(x)
    output = Dense(1)(x)

    model = Model(inputs=[input_origen, input_destino, input_rango_horario, input_salida, input_llegada, input_features], outputs=output)
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

    model.fit(
        [X_train['Origen_int'], X_train['Destino_int'], X_train['Rango_Horario_Consulta_int'],
         X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
         X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
        y_train,
        validation_data=([
            X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
            X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
            X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values], y_val),
        epochs=100,
        batch_size=64,
        verbose=0
    )

    y_pred = model.predict([
        X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
        X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
        X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
    ])

    return mean_squared_error(y_val, y_pred)

# --- Optimizaci√≥n
start_time = time.time()
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)
end_time = time.time()

# --- Resultados
print("\nüìå Mejores hiperpar√°metros encontrados:")
for k, v in study.best_params.items():
    print(f"  - {k}: {v}")
print(f"\n‚è±Ô∏è Tiempo total de optimizaci√≥n: {end_time - start_time:.2f} segundos")

# --- Reconstrucci√≥n del mejor modelo para obtener m√©tricas
best_params = study.best_params

# Reconstrucci√≥n del modelo con los mejores hiperpar√°metros
embedding_dim = best_params['embedding_dim']
lstm_units = best_params['lstm_units']
dense_units = best_params['dense_units']
dropout_rate = best_params['dropout']
learning_rate = best_params['learning_rate']

input_origen = Input(shape=(1,))
input_destino = Input(shape=(1,))
input_rango_horario = Input(shape=(1,))
input_salida = Input(shape=(1,))
input_llegada = Input(shape=(1,))
input_features = Input(shape=(3,))

embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
embedding_rango_horario = Embedding(input_dim=X['Rango_Horario_Consulta_int'].nunique()+1, output_dim=embedding_dim)(input_rango_horario)
embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

x = Concatenate()([
    Flatten()(embedding_origen),
    Flatten()(embedding_destino),
    Flatten()(embedding_rango_horario),
    Flatten()(embedding_salida),
    Flatten()(embedding_llegada),
    input_features
])

x = BatchNormalization()(x)
x = Dropout(dropout_rate)(x)
x = Reshape((1, x.shape[-1]))(x)
x = LSTM(lstm_units)(x)
x = Dense(dense_units, activation='relu')(x)
output = Dense(1)(x)

final_model = Model(inputs=[input_origen, input_destino, input_rango_horario, input_salida, input_llegada, input_features], outputs=output)
final_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

final_model.fit(
    [X_train['Origen_int'], X_train['Destino_int'], X_train['Rango_Horario_Consulta_int'],
     X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
     X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
    y_train,
    epochs=100,
    batch_size=64,
    verbose=0
)

y_pred_final = final_model.predict([
    X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
    X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
    X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
])

# --- M√©tricas finales
mae = mean_absolute_error(y_val, y_pred_final)
mse = mean_squared_error(y_val, y_pred_final)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, y_pred_final)

print("\nüìä M√©tricas del mejor modelo encontrado:")
print(f"  - MAE:  {mae:.4f}")
print(f"  - MSE:  {mse:.4f}")
print(f"  - RMSE: {rmse:.4f}")
print(f"  - R¬≤:   {r2:.4f}")
# -------------------------
# --- Guardar tiempo total de entrenamiento de Optuna
tiempo_entrenamiento = end_time - start_time
# Despu√©s de entrenar y obtener m√©tricas para el modelo lstm (Objetivo 2):
resultados.agregar_resultados_objetivo2('LSTM', mae, mse, rmse, r2, tiempo_entrenamiento)
resultados.agregar_hiperparametros_obj2('LSTM', best_params)

"""## **E2 - MODELO 2: GRU**"""

import optuna
import time
import numpy as np
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Dropout, BatchNormalization, Concatenate, Flatten, Reshape
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# --- Divisi√≥n de datos
X = df[['Origen_int', 'Destino_int', 'Duracion_minutos', 'Directo_binario',
        'Tarifa', 'Rango_Horario_Consulta_int', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Dias_hasta_vuelo']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos', 'Tarifa']] = scaler.fit_transform(X_train[['Duracion_minutos', 'Tarifa']])
X_val[['Duracion_minutos', 'Tarifa']] = scaler.transform(X_val[['Duracion_minutos', 'Tarifa']])

# --- Optuna objective
def objective(trial):
    embedding_dim = trial.suggest_int('embedding_dim', 4, 16)
    gru_units = trial.suggest_int('gru_units', 32, 128)
    dense_units = trial.suggest_int('dense_units', 32, 128)
    dropout_rate = trial.suggest_float('dropout', 0.1, 0.5)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)

    # Entradas
    input_origen = Input(shape=(1,))
    input_destino = Input(shape=(1,))
    input_rango_horario = Input(shape=(1,))
    input_salida = Input(shape=(1,))
    input_llegada = Input(shape=(1,))
    input_features = Input(shape=(3,))

    embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
    embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
    embedding_rango_horario = Embedding(input_dim=X['Rango_Horario_Consulta_int'].nunique()+1, output_dim=embedding_dim)(input_rango_horario)
    embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
    embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

    x = Concatenate()([
        Flatten()(embedding_origen),
        Flatten()(embedding_destino),
        Flatten()(embedding_rango_horario),
        Flatten()(embedding_salida),
        Flatten()(embedding_llegada),
        input_features
    ])

    x = Reshape((1, x.shape[-1]))(x)
    x = BatchNormalization()(x)
    x = GRU(gru_units, return_sequences=False)(x)
    x = Dense(dense_units, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    output = Dense(1)(x)

    model = Model(inputs=[input_origen, input_destino, input_rango_horario, input_salida, input_llegada, input_features], outputs=output)
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

    model.fit(
        [X_train['Origen_int'], X_train['Destino_int'], X_train['Rango_Horario_Consulta_int'],
         X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
         X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
        y_train,
        validation_data=([
            X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
            X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
            X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values], y_val),
        epochs=100,
        batch_size=64,
        verbose=0
    )

    y_pred = model.predict([
        X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
        X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
        X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
    ])

    return mean_squared_error(y_val, y_pred)

# --- Ejecutar Optuna
print("‚è≥ Optimizando modelo GRU con Optuna...")
start_time = time.time()
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)
opt_time = time.time() - start_time

# --- Hiperpar√°metros √≥ptimos
best_params = study.best_params
print("\nüìå Mejores hiperpar√°metros GRU:")
for k, v in best_params.items():
    print(f"  - {k}: {v}")
print(f"\n‚è±Ô∏è Tiempo de optimizaci√≥n: {opt_time:.2f} segundos")

# --- Reconstrucci√≥n del mejor modelo
embedding_dim = best_params['embedding_dim']
gru_units = best_params['gru_units']
dense_units = best_params['dense_units']
dropout_rate = best_params['dropout']
learning_rate = best_params['learning_rate']

# Entradas
input_origen = Input(shape=(1,))
input_destino = Input(shape=(1,))
input_rango_horario = Input(shape=(1,))
input_salida = Input(shape=(1,))
input_llegada = Input(shape=(1,))
input_features = Input(shape=(3,))

embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
embedding_rango_horario = Embedding(input_dim=X['Rango_Horario_Consulta_int'].nunique()+1, output_dim=embedding_dim)(input_rango_horario)
embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

x = Concatenate()([
    Flatten()(embedding_origen),
    Flatten()(embedding_destino),
    Flatten()(embedding_rango_horario),
    Flatten()(embedding_salida),
    Flatten()(embedding_llegada),
    input_features
])

x = Reshape((1, x.shape[-1]))(x)
x = BatchNormalization()(x)
x = GRU(gru_units, return_sequences=False)(x)
x = Dense(dense_units, activation='relu')(x)
x = Dropout(dropout_rate)(x)
output = Dense(1)(x)

model_gru_best = Model(inputs=[input_origen, input_destino, input_rango_horario, input_salida, input_llegada, input_features], outputs=output)
model_gru_best.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

# --- Entrenamiento final
print("\nüìà Entrenando mejor modelo GRU...")
start_time = time.time()
model_gru_best.fit(
    [X_train['Origen_int'], X_train['Destino_int'], X_train['Rango_Horario_Consulta_int'],
     X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
     X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
    y_train,
    epochs=100,
    batch_size=64,
    verbose=0
)
train_time = time.time() - start_time

# --- Evaluaci√≥n
print("\nüìä Evaluando mejor modelo GRU...")
y_pred = model_gru_best.predict([
    X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
    X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
    X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
])

mae = mean_absolute_error(y_val, y_pred)
mse = mean_squared_error(y_val, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, y_pred)

print(f"\n‚úÖ GRU Metrics:")
print(f"  - MAE:  {mae:.4f}")
print(f"  - MSE:  {mse:.4f}")
print(f"  - RMSE: {rmse:.4f}")
print(f"  - R¬≤:   {r2:.4f}")
print(f"  - Tiempo de entrenamiento: {train_time:.2f} seg")
# -------------------------
# --- Guardar tiempo total de entrenamiento de Optuna
tiempo_entrenamiento = end_time - start_time
# Despu√©s de entrenar y obtener m√©tricas para el modelo GRU (Objetivo 2):
resultados.agregar_resultados_objetivo2('GRU', mae, mse, rmse, r2, opt_time)
resultados.agregar_hiperparametros_obj2('GRU', best_params)

import optuna
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Lambda, Conv1D, Dropout, BatchNormalization, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split

"""## **E2 - MODELO 3: TCN**"""

import optuna
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Lambda, Conv1D, Dropout, BatchNormalization, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# --- Preparaci√≥n de datos
X = df[['Origen_int', 'Destino_int', 'Duracion_minutos', 'Directo_binario',
        'Tarifa', 'Rango_Horario_Consulta_int', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Dias_hasta_vuelo']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos', 'Tarifa']] = scaler.fit_transform(X_train[['Duracion_minutos', 'Tarifa']])
X_val[['Duracion_minutos', 'Tarifa']] = scaler.transform(X_val[['Duracion_minutos', 'Tarifa']])

# --- Funci√≥n de Optuna
def objective(trial):
    embedding_dim = trial.suggest_int('embedding_dim', 4, 16)
    filters = trial.suggest_int('filters', 32, 128)
    dropout_rate = trial.suggest_float('dropout', 0.1, 0.5)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)

    # Definici√≥n de entradas
    input_origen = Input(shape=(1,))
    input_destino = Input(shape=(1,))
    input_rango_horario = Input(shape=(1,))
    input_salida = Input(shape=(1,))
    input_llegada = Input(shape=(1,))
    input_features = Input(shape=(3,))

    # Embeddings
    embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
    embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
    embedding_rango_horario = Embedding(input_dim=X['Rango_Horario_Consulta_int'].nunique()+1, output_dim=embedding_dim)(input_rango_horario)
    embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
    embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

    x_embeddings = Concatenate()([
        Flatten()(embedding_origen),
        Flatten()(embedding_destino),
        Flatten()(embedding_rango_horario),
        Flatten()(embedding_salida),
        Flatten()(embedding_llegada)
    ])

    x = Concatenate()([x_embeddings, input_features])
    x = Lambda(lambda t: tf.expand_dims(t, axis=1))(x)

    x = BatchNormalization()(x)
    x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=1)(x)
    x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=2)(x)
    x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=4)(x)
    x = Dropout(dropout_rate)(x)
    x = Flatten()(x)
    output = Dense(1)(x)

    model = Model(inputs=[input_origen, input_destino, input_rango_horario, input_salida, input_llegada, input_features], outputs=output)
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

    model.fit(
        [X_train['Origen_int'], X_train['Destino_int'], X_train['Rango_Horario_Consulta_int'],
         X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
         X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
        y_train,
        validation_data=([X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
                          X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
                          X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values], y_val),
        epochs=100,
        batch_size=64,
        verbose=0
    )

    y_pred = model.predict([
        X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
        X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
        X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
    ])

    return mean_squared_error(y_val, y_pred)

# --- Ejecuci√≥n de Optuna
print("‚è≥ Optimizando modelo TCN con Optuna...")
start_opt = time.time()
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)
opt_time = time.time() - start_opt

# --- Mejores hiperpar√°metros
best_params = study.best_params
print("\nüìå Mejores hiperpar√°metros TCN:")
for k, v in best_params.items():
    print(f"  - {k}: {v}")
print(f"\n‚è±Ô∏è Tiempo de optimizaci√≥n: {opt_time:.2f} segundos")

# --- Reconstrucci√≥n del mejor modelo
embedding_dim = best_params['embedding_dim']
filters = best_params['filters']
dropout_rate = best_params['dropout']
learning_rate = best_params['learning_rate']

# Redefinir inputs
input_origen = Input(shape=(1,))
input_destino = Input(shape=(1,))
input_rango_horario = Input(shape=(1,))
input_salida = Input(shape=(1,))
input_llegada = Input(shape=(1,))
input_features = Input(shape=(3,))

# Embeddings
embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
embedding_rango_horario = Embedding(input_dim=X['Rango_Horario_Consulta_int'].nunique()+1, output_dim=embedding_dim)(input_rango_horario)
embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

x_embeddings = Concatenate()([
    Flatten()(embedding_origen),
    Flatten()(embedding_destino),
    Flatten()(embedding_rango_horario),
    Flatten()(embedding_salida),
    Flatten()(embedding_llegada)
])

x = Concatenate()([x_embeddings, input_features])
x = Lambda(lambda t: tf.expand_dims(t, axis=1))(x)

x = BatchNormalization()(x)
x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=1)(x)
x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=2)(x)
x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=4)(x)
x = Dropout(dropout_rate)(x)
x = Flatten()(x)
output = Dense(1)(x)

model_tcn_best = Model(inputs=[input_origen, input_destino, input_rango_horario, input_salida, input_llegada, input_features], outputs=output)
model_tcn_best.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

# --- Entrenamiento final
print("\nüìà Entrenando mejor modelo TCN...")
start_train = time.time()
model_tcn_best.fit(
    [X_train['Origen_int'], X_train['Destino_int'], X_train['Rango_Horario_Consulta_int'],
     X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
     X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
    y_train,
    epochs=100,
    batch_size=64,
    verbose=0
)
train_time = time.time() - start_train

# --- Evaluaci√≥n
print("\nüìä Evaluando mejor modelo TCN...")
y_pred = model_tcn_best.predict([
    X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
    X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
    X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
])

mae = mean_absolute_error(y_val, y_pred)
mse = mean_squared_error(y_val, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, y_pred)

print(f"\n‚úÖ TCN Metrics:")
print(f"  - MAE:  {mae:.4f}")
print(f"  - MSE:  {mse:.4f}")
print(f"  - RMSE: {rmse:.4f}")
print(f"  - R¬≤:   {r2:.4f}")
print(f"  - Tiempo de entrenamiento (guardado): {train_time:.2f} seg")

# --- Guardar resultados
total_time = opt_time + train_time
resultados.agregar_resultados_objetivo2('TCN', mae, mse, rmse, r2, total_time)
resultados.agregar_hiperparametros_obj2('TCN', best_params)

"""## **E2 - MODELO 4: TRANSFORMERS**"""

import optuna
import time
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Dropout, Flatten, Concatenate, Lambda, MultiHeadAttention, LayerNormalization
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# --- Preparaci√≥n de datos
X = df[['Origen_int', 'Destino_int', 'Duracion_minutos', 'Directo_binario',
        'Tarifa', 'Rango_Horario_Consulta_int', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Dias_hasta_vuelo']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos', 'Tarifa']] = scaler.fit_transform(X_train[['Duracion_minutos', 'Tarifa']])
X_val[['Duracion_minutos', 'Tarifa']] = scaler.transform(X_val[['Duracion_minutos', 'Tarifa']])

# --- Inicia medici√≥n de tiempo total
start_total = time.time()

# --- Optuna
def objective(trial):
    embedding_dim = trial.suggest_int("embedding_dim", 4, 16)
    d_model = trial.suggest_int("d_model", 32, 128)
    num_heads = trial.suggest_int("num_heads", 2, 8)
    dropout_rate = trial.suggest_float("dropout_rate", 0.1, 0.5)
    learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2)

    input_origen = Input(shape=(1,))
    input_destino = Input(shape=(1,))
    input_rango_horario = Input(shape=(1,))
    input_salida = Input(shape=(1,))
    input_llegada = Input(shape=(1,))
    input_features = Input(shape=(3,))

    embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
    embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
    embedding_rango_horario = Embedding(input_dim=X['Rango_Horario_Consulta_int'].nunique()+1, output_dim=embedding_dim)(input_rango_horario)
    embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
    embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

    x_embeddings = Concatenate()([
        Flatten()(embedding_origen),
        Flatten()(embedding_destino),
        Flatten()(embedding_rango_horario),
        Flatten()(embedding_salida),
        Flatten()(embedding_llegada)
    ])

    x = Concatenate()([x_embeddings, input_features])
    x = Dense(d_model, activation='relu')(x)
    x = Dropout(dropout_rate)(x)
    x = Lambda(lambda t: tf.expand_dims(t, axis=1))(x)

    attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)
    attn_output = Dropout(dropout_rate)(attn_output)
    attn_output = LayerNormalization(epsilon=1e-6)(x + attn_output)

    ffn = Dense(d_model, activation='relu')(attn_output)
    ffn = Dropout(dropout_rate)(ffn)
    ffn = Dense(d_model, activation='relu')(ffn)
    ffn = Dropout(dropout_rate)(ffn)
    x = LayerNormalization(epsilon=1e-6)(attn_output + ffn)

    x = Flatten()(x)
    output = Dense(1)(x)

    model = Model(inputs=[input_origen, input_destino, input_rango_horario, input_salida, input_llegada, input_features], outputs=output)
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

    model.fit(
        [X_train['Origen_int'], X_train['Destino_int'], X_train['Rango_Horario_Consulta_int'],
         X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
         X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
        y_train,
        validation_data=([X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
                          X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
                          X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values], y_val),
        epochs=100,
        batch_size=64,
        verbose=0
    )

    y_pred = model.predict([
        X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
        X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
        X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
    ])
    return mean_squared_error(y_val, y_pred)

# --- Ejecutar Optuna
print("‚è≥ Optimizando modelo Transformer con Optuna...")
start_opt = time.time()
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)
opt_time = time.time() - start_opt

# --- Mejores hiperpar√°metros
best_params = study.best_params
print("\nüìå Mejores hiperpar√°metros Transformer:")
for k, v in best_params.items():
    print(f"  - {k}: {v}")
print(f"\n‚è±Ô∏è Tiempo de optimizaci√≥n: {opt_time:.2f} segundos")

# --- Reconstrucci√≥n del mejor modelo
embedding_dim = best_params['embedding_dim']
d_model = best_params['d_model']
num_heads = best_params['num_heads']
dropout_rate = best_params['dropout_rate']
learning_rate = best_params['learning_rate']

input_origen = Input(shape=(1,))
input_destino = Input(shape=(1,))
input_rango_horario = Input(shape=(1,))
input_salida = Input(shape=(1,))
input_llegada = Input(shape=(1,))
input_features = Input(shape=(3,))

embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
embedding_rango_horario = Embedding(input_dim=X['Rango_Horario_Consulta_int'].nunique()+1, output_dim=embedding_dim)(input_rango_horario)
embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

x_embeddings = Concatenate()([
    Flatten()(embedding_origen),
    Flatten()(embedding_destino),
    Flatten()(embedding_rango_horario),
    Flatten()(embedding_salida),
    Flatten()(embedding_llegada)
])

x = Concatenate()([x_embeddings, input_features])
x = Dense(d_model, activation='relu')(x)
x = Dropout(dropout_rate)(x)
x = Lambda(lambda t: tf.expand_dims(t, axis=1))(x)

attn_output = MultiHeadAttention(num_heads=num_heads, key_dim=d_model)(x, x)
attn_output = Dropout(dropout_rate)(attn_output)
attn_output = LayerNormalization(epsilon=1e-6)(x + attn_output)

ffn = Dense(d_model, activation='relu')(attn_output)
ffn = Dropout(dropout_rate)(ffn)
ffn = Dense(d_model, activation='relu')(ffn)
ffn = Dropout(dropout_rate)(ffn)
x = LayerNormalization(epsilon=1e-6)(attn_output + ffn)

x = Flatten()(x)
output = Dense(1)(x)

model_trans_best = Model(inputs=[input_origen, input_destino, input_rango_horario, input_salida, input_llegada, input_features], outputs=output)
model_trans_best.compile(optimizer=Adam(learning_rate=learning_rate), loss='mse')

# --- Entrenamiento final
print("\nüìà Entrenando mejor modelo Transformer...")
start_train = time.time()
model_trans_best.fit(
    [X_train['Origen_int'], X_train['Destino_int'], X_train['Rango_Horario_Consulta_int'],
     X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
     X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
    y_train,
    epochs=100,
    batch_size=64,
    verbose=0
)
train_time = time.time() - start_train

# --- Evaluaci√≥n
print("\nüìä Evaluando mejor modelo Transformer...")
y_pred = model_trans_best.predict([
    X_val['Origen_int'], X_val['Destino_int'], X_val['Rango_Horario_Consulta_int'],
    X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
    X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
])

mae = mean_absolute_error(y_val, y_pred)
mse = mean_squared_error(y_val, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, y_pred)

# --- Tiempo total
total_time = time.time() - start_total

print(f"\n‚úÖ Transformer Metrics:")
print(f"  - MAE:  {mae:.4f}")
print(f"  - MSE:  {mse:.4f}")
print(f"  - RMSE: {rmse:.4f}")
print(f"  - R¬≤:   {r2:.4f}")
print(f"‚è±Ô∏è Tiempo de entrenamiento: {train_time:.2f} seg")
print(f"‚è±Ô∏è Tiempo total de ejecuci√≥n (Optuna + entrenamiento): {total_time:.2f} seg")
# -------------------------
# Despu√©s de entrenar y obtener m√©tricas para el modelo TRANSFORMERS(Objetivo 2):
resultados.agregar_resultados_objetivo2('TRANSFORMERS', mae, mse, rmse, r2, total_time)
resultados.agregar_hiperparametros_obj2('TRANSFORMERS', best_params)

"""## Definici√≥n de los embeddings para los modelos cl√°sicos"""

from tensorflow.keras.layers import Embedding, Input, Reshape
from tensorflow.keras.models import Model
import tensorflow as tf

# Variables categ√≥ricas con embedding
origen_input = Input(shape=(1,))
destino_input = Input(shape=(1,))
directo_input = Input(shape=(1,))
hora_salida_input = Input(shape=(1,))
hora_llegada_input = Input(shape=(1,))

origen_emb = Embedding(input_dim=df['Origen_int'].nunique()+1, output_dim=4)(origen_input)
destino_emb = Embedding(input_dim=df['Destino_int'].nunique()+1, output_dim=4)(destino_input)
directo_emb = Embedding(input_dim=2, output_dim=2)(directo_input)
hora_salida_emb = Embedding(input_dim=df['Hora_Salida_Rango_int'].nunique()+1, output_dim=3)(hora_salida_input)
hora_llegada_emb = Embedding(input_dim=df['Hora_Llegada_Rango_int'].nunique()+1, output_dim=3)(hora_llegada_input)

# Aplanar los vectores
origen_flat = Reshape((4,))(origen_emb)
destino_flat = Reshape((4,))(destino_emb)
directo_flat = Reshape((2,))(directo_emb)
hora_salida_flat = Reshape((3,))(hora_salida_emb)
hora_llegada_flat = Reshape((3,))(hora_llegada_emb)

# Concatenar todos
inputs = tf.keras.layers.Concatenate()([
    origen_flat, destino_flat, directo_flat, hora_salida_flat, hora_llegada_flat
])

"""## **E2 - MODELO 5: RANDOM FOREST**"""

import time
import optuna
import numpy as np
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Flatten
from tensorflow.keras.models import Model

# --- Datos
cat_cols = ['Origen_int', 'Destino_int', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']
num_cols = ['Duracion_minutos', 'Directo_binario']
target_col = 'Dias_hasta_vuelo'

X = df[cat_cols + num_cols]
y = df[target_col]

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Escalar solo num√©ricos
scaler = StandardScaler()
X_train[num_cols] = scaler.fit_transform(X_train[num_cols])
X_val[num_cols] = scaler.transform(X_val[num_cols])

# --- Entrenar embeddings como modelo auxiliar
embedding_dim = 4  # Fijo o parametrizable

inputs = []
embeddings = []

for col in cat_cols:
    inp = Input(shape=(1,), name=f"{col}_input")
    vocab_size = df[col].nunique() + 1
    emb = Embedding(input_dim=vocab_size, output_dim=embedding_dim, name=f"{col}_emb")(inp)
    emb = Flatten()(emb)
    inputs.append(inp)
    embeddings.append(emb)

model_emb = Model(inputs=inputs, outputs=embeddings)
# Este modelo no se entrena, solo se usar√° para mapear los √≠ndices a vectores

# --- Transformar datos categ√≥ricos en embeddings
def embed_batch(X_cat_df):
    input_data = [X_cat_df[[col]].values for col in cat_cols]
    return model_emb.predict(input_data, verbose=0)

X_train_emb = embed_batch(X_train[cat_cols])
X_val_emb = embed_batch(X_val[cat_cols])

# --- Concatenar todo
X_train_final = np.concatenate(X_train_emb + [X_train[num_cols].values], axis=1)
X_val_final = np.concatenate(X_val_emb + [X_val[num_cols].values], axis=1)

# --- Tiempo total
start_total = time.time()

# --- Optuna para Random Forest
def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 100, 100)
    max_depth = trial.suggest_int('max_depth', 3, 10)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 5)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)
    max_features = trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])

    model_rf = RandomForestRegressor(
        random_state=42,
        n_estimators=n_estimators,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features
    )

    model_rf.fit(X_train_final, y_train)
    y_pred = model_rf.predict(X_val_final)
    return mean_squared_error(y_val, y_pred)

print("‚è≥ Optimizando Random Forest con Optuna (con embeddings)...")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)

# --- Mejor modelo
best_params = study.best_params
print(f"\nüìå Mejores par√°metros encontrados: {best_params}")

model_rf_best = RandomForestRegressor(random_state=42, **best_params)
model_rf_best.fit(X_train_final, y_train)

# --- Evaluaci√≥n
y_pred_final = model_rf_best.predict(X_val_final)

mae = mean_absolute_error(y_val, y_pred_final)
mse = mean_squared_error(y_val, y_pred_final)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, y_pred_final)
tiempo_total = time.time() - start_total

# --- Reporte
print(f"\n‚úÖ Random Forest (con embeddings) Metrics:")
print(f"  - MAE:  {mae:.4f}")
print(f"  - MSE:  {mse:.4f}")
print(f"  - RMSE: {rmse:.4f}")
print(f"  - R¬≤:   {r2:.4f}")
print(f"‚è±Ô∏è Tiempo total de ejecuci√≥n: {tiempo_total:.2f} seg")

# --- Guardar resultados
resultados.agregar_resultados_objetivo2('Random Forest', mae, mse, rmse, r2, tiempo_total)
resultados.agregar_hiperparametros_obj2('Random Forest', best_params)

"""## **E2 - MODELO 6: ARBOL DE DECISI√ìN**"""

import optuna
import time
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeRegressor
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Supongamos que df ya tiene los embeddings agregados como columnas
# Ejemplo de columnas generadas por embeddings: Origen_emb_0, Origen_emb_1, ..., Destino_emb_0, ...
columnas_embeddings = [col for col in df.columns if col.startswith('Origen_emb_') or col.startswith('Destino_emb_')]

# --- Preparaci√≥n de datos
X = df[columnas_embeddings + ['Duracion_minutos', 'Directo_binario', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Dias_hasta_vuelo']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos']] = scaler.fit_transform(X_train[['Duracion_minutos']])
X_val[['Duracion_minutos']] = scaler.transform(X_val[['Duracion_minutos']])

# --- Tiempo total
start_total = time.time()

# --- Funci√≥n objetivo para Optuna
def objective(trial):
    max_depth = trial.suggest_int('max_depth', 100, 100)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 5)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)
    max_features = trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])

    model = DecisionTreeRegressor(
        random_state=42,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features
    )
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    mse = mean_squared_error(y_val, y_pred)
    return mse

# --- Ejecutar Optuna (1 trial)
print("‚è≥ Optimizando Decision Tree con Optuna...")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)

# --- Mejor modelo con hiperpar√°metros √≥ptimos
best_params = study.best_params
print("\nüìå Mejores hiperpar√°metros Decision Tree:")
for k, v in best_params.items():
    print(f"  - {k}: {v}")

# --- Entrenamiento
print("\nüìà Entrenando mejor modelo Decision Tree (ArbolD)...")
model = DecisionTreeRegressor(random_state=42, **best_params)
model.fit(X_train, y_train)

# --- Evaluaci√≥n
print("\nüìä Evaluando mejor modelo Decision Tree (ArbolD)...")
y_pred = model.predict(X_val)
mae = mean_absolute_error(y_val, y_pred)
mse = mean_squared_error(y_val, y_pred)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, y_pred)

tiempo_total = time.time() - start_total

# --- Resultados
print(f"\n‚úÖ Decision Tree Metrics (ArbolD):")
print(f"  - MAE:  {mae:.4f}")
print(f"  - MSE:  {mse:.4f}")
print(f"  - RMSE: {rmse:.4f}")
print(f"  - R¬≤:   {r2:.4f}")
print(f"‚è±Ô∏è Tiempo total de ejecuci√≥n: {tiempo_total:.2f} seg")

# --- Guardar resultados
resultados.agregar_resultados_objetivo2('Arbol de Decision', mae, mse, rmse, r2, tiempo_total)
resultados.agregar_hiperparametros_obj2('Arbol de Decision', best_params)

"""## **E2 - MODELO 7: KNN**"""

import time
import optuna
import numpy as np
import pandas as pd
from sklearn.neighbors import KNeighborsRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate
from tensorflow.keras.utils import plot_model

# --- Datos originales
categorical_features = ['Origen_int', 'Destino_int', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']
numerical_features = ['Duracion_minutos', 'Directo_binario']
target = 'Dias_hasta_vuelo'

# --- Preprocesamiento
X_cat = df[categorical_features].copy()
X_num = df[numerical_features].copy()
y = df[target].copy()

# Normalizar la columna num√©rica
scaler = StandardScaler()
X_num[['Duracion_minutos']] = scaler.fit_transform(X_num[['Duracion_minutos']])

# Embeddings para variables categ√≥ricas
inputs = []
embeddings = []

for col in categorical_features:
    input_cat = Input(shape=(1,), name=f'{col}_input')
    vocab_size = df[col].nunique()
    embed_dim = min(50, vocab_size // 2 + 1)
    embed = Embedding(input_dim=vocab_size + 1, output_dim=embed_dim, name=f'{col}_embedding')(input_cat)
    embed = Flatten()(embed)
    inputs.append(input_cat)
    embeddings.append(embed)

# Concatenar todo
input_num = Input(shape=(X_num.shape[1],), name='numerical_input')
inputs.append(input_num)
x = Concatenate()(embeddings + [input_num])

# Crear modelo para generar X_embed
embedding_model = Model(inputs=inputs, outputs=x)

# Preparar entradas para embeddings
X_inputs = [X_cat[col].values for col in categorical_features] + [X_num.values]
X_embed = embedding_model.predict(X_inputs)

# Dividir
X_train, X_val, y_train, y_val = train_test_split(X_embed, y, test_size=0.2, random_state=42)

# --- Medir tiempo total
start_total = time.time()

def objective(trial):
    n_neighbors = trial.suggest_int('n_neighbors', 1, 20)
    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])
    p = trial.suggest_int('p', 1, 2)

    model = KNeighborsRegressor(n_neighbors=n_neighbors, weights=weights, p=p)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    return mean_squared_error(y_val, y_pred)

print("‚è≥ Optimizando KNN con Optuna...")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10, n_jobs=10)

best_params = study.best_params
print("\nüìå Mejores par√°metros KNN:")
for k, v in best_params.items():
    print(f"  - {k}: {v}")

# --- Entrenamiento final
print("\nüìà Entrenando mejor modelo KNN...")
start_train = time.time()
knn_final = KNeighborsRegressor(**best_params)
knn_final.fit(X_train, y_train)
train_time = time.time() - start_train

# --- Evaluaci√≥n
print("\nüìä Evaluando mejor modelo KNN...")
y_pred_final = knn_final.predict(X_val)

mae = mean_absolute_error(y_val, y_pred_final)
mse = mean_squared_error(y_val, y_pred_final)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, y_pred_final)
total_time = time.time() - start_total

print(f"\n‚úÖ KNN Metrics:")
print(f"  - MAE:  {mae:.4f}")
print(f"  - MSE:  {mse:.4f}")
print(f"  - RMSE: {rmse:.4f}")
print(f"  - R¬≤:   {r2:.4f}")
print(f"‚è±Ô∏è Tiempo total de ejecuci√≥n: {total_time:.2f} seg")

# --- Guardar resultados
resultados.agregar_resultados_objetivo2('KNN', mae, mse, rmse, r2, total_time)
resultados.agregar_hiperparametros_obj2('KNN', best_params)

"""## **E2 - MODELO 8: XGBoost**"""

import time
import optuna
from xgboost import XGBRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
import numpy as np

# --- Seleccionar columnas de embeddings y otras variables
columnas_embeddings = [col for col in df.columns if col.startswith('Origen_emb_') or col.startswith('Destino_emb_')]

X = df[columnas_embeddings + ['Duracion_minutos', 'Directo_binario', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Dias_hasta_vuelo']

# --- Separaci√≥n y escalado
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos']] = scaler.fit_transform(X_train[['Duracion_minutos']])
X_val[['Duracion_minutos']] = scaler.transform(X_val[['Duracion_minutos']])

# --- Optuna
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 100),
        'max_depth': trial.suggest_int('max_depth', 100, 100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 5),
        'random_state': 42,
        'use_label_encoder': False,
        'eval_metric': 'rmse',
    }

    model = XGBRegressor(**params)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    mse = mean_squared_error(y_val, y_pred)
    return mse

# --- Tiempo total
start_total = time.time()

print("‚è≥ Optimizando XGBoost con Optuna...")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)

opt_time = time.time() - start_total

# --- Mejor modelo
best_params = study.best_params
best_params.update({
    'random_state': 42,
    'use_label_encoder': False,
    'eval_metric': 'rmse'
})

print("\nüìå Mejores hiperpar√°metros XGBoost:")
for k, v in best_params.items():
    print(f"  - {k}: {v}")
print(f"\n‚è±Ô∏è Tiempo total (Optuna + entrenamiento): {opt_time:.2f} segundos")

# --- Entrenamiento y evaluaci√≥n
model_xgb = XGBRegressor(**best_params)
model_xgb.fit(X_train, y_train)

y_pred_final = model_xgb.predict(X_val)
mae = mean_absolute_error(y_val, y_pred_final)
mse = mean_squared_error(y_val, y_pred_final)
rmse = np.sqrt(mse)
r2 = r2_score(y_val, y_pred_final)

print(f"\n‚úÖ XGBoost Metrics:")
print(f"  - MAE:  {mae:.4f}")
print(f"  - MSE:  {mse:.4f}")
print(f"  - RMSE: {rmse:.4f}")
print(f"  - R¬≤:   {r2:.4f}")

# --- Guardar resultado
resultados.agregar_resultados_objetivo2('XGBoost', mae, mse, rmse, r2, opt_time)
resultados.agregar_hiperparametros_obj2('XGBoost', best_params)

"""# **ENTRENAMIENTO 3: OBJETIVO PREDECIR RANGO HORARIO OPTIMO**

## **E3 - MODELO 1: LSTM**
"""

import optuna
import time
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, LSTM, Concatenate, Dropout, BatchNormalization, Reshape, Flatten
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import numpy as np

# --- Medir tiempo total
start_time = time.time()

# --- Divisi√≥n de datos
X = df[['Origen_int', 'Destino_int', 'Duracion_minutos', 'Directo_binario',
        'Tarifa', 'Dias_hasta_vuelo', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Rango_Horario_Consulta_int']  # OBJETIVO CAMBIADO

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos', 'Tarifa', 'Dias_hasta_vuelo']] = scaler.fit_transform(X_train[['Duracion_minutos', 'Tarifa', 'Dias_hasta_vuelo']])
X_val[['Duracion_minutos', 'Tarifa', 'Dias_hasta_vuelo']] = scaler.transform(X_val[['Duracion_minutos', 'Tarifa', 'Dias_hasta_vuelo']])

# --- Optuna objective
def objective(trial):
    embedding_dim = trial.suggest_int('embedding_dim', 4, 16)
    lstm_units = trial.suggest_int('lstm_units', 32, 128)
    dense_units = trial.suggest_int('dense_units', 32, 128)
    dropout_rate = trial.suggest_float('dropout', 0.1, 0.5)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2)

    # Entradas categ√≥ricas
    input_origen = Input(shape=(1,))
    input_destino = Input(shape=(1,))
    input_salida = Input(shape=(1,))
    input_llegada = Input(shape=(1,))
    input_features = Input(shape=(3,))  # Duracion, Directo, Tarifa

    embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
    embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
    embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
    embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

    x = Concatenate()([
        Flatten()(embedding_origen),
        Flatten()(embedding_destino),
        Flatten()(embedding_salida),
        Flatten()(embedding_llegada),
        input_features
    ])

    x = BatchNormalization()(x)
    x = Dropout(dropout_rate)(x)
    x = Reshape((1, x.shape[-1]))(x)
    x = LSTM(lstm_units)(x)
    x = Dense(dense_units, activation='relu')(x)
    output = Dense(y.nunique(), activation='softmax')(x)  # softmax para clasificaci√≥n

    model = Model(inputs=[input_origen, input_destino, input_salida, input_llegada, input_features], outputs=output)
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    model.fit(
        [X_train['Origen_int'], X_train['Destino_int'], X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
         X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
        y_train,
        validation_data=([X_val['Origen_int'], X_val['Destino_int'], X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
                          X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values], y_val),
        epochs=100,
        batch_size=64,
        verbose=0
    )

    y_pred_probs = model.predict([
        X_val['Origen_int'], X_val['Destino_int'], X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
        X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
    ])
    y_pred = np.argmax(y_pred_probs, axis=1)

    return accuracy_score(y_val, y_pred)

# --- Optuna
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=10)
end_time = time.time()

# --- Reconstrucci√≥n final del mejor modelo
params = study.best_params
embedding_dim = params['embedding_dim']
lstm_units = params['lstm_units']
dense_units = params['dense_units']
dropout_rate = params['dropout']
learning_rate = params['learning_rate']

# Modelo final
input_origen = Input(shape=(1,))
input_destino = Input(shape=(1,))
input_salida = Input(shape=(1,))
input_llegada = Input(shape=(1,))
input_features = Input(shape=(3,))

embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

x = Concatenate()([
    Flatten()(embedding_origen),
    Flatten()(embedding_destino),
    Flatten()(embedding_salida),
    Flatten()(embedding_llegada),
    input_features
])
x = BatchNormalization()(x)
x = Dropout(dropout_rate)(x)
x = Reshape((1, x.shape[-1]))(x)
x = LSTM(lstm_units)(x)
x = Dense(dense_units, activation='relu')(x)
output = Dense(y.nunique(), activation='softmax')(x)

final_model = Model(inputs=[input_origen, input_destino, input_salida, input_llegada, input_features], outputs=output)
final_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

final_model.fit(
    [X_train['Origen_int'], X_train['Destino_int'], X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
     X_train[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values],
    y_train,
    epochs=100,
    batch_size=64,
    verbose=0
)

# --- Predicci√≥n y m√©tricas
y_pred_probs = final_model.predict([
    X_val['Origen_int'], X_val['Destino_int'], X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
    X_val[['Duracion_minutos', 'Directo_binario', 'Tarifa']].values
])
y_pred = np.argmax(y_pred_probs, axis=1)

accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted')
recall = recall_score(y_val, y_pred, average='weighted')
f1 = f1_score(y_val, y_pred, average='weighted')

print("\nüìä M√©tricas del mejor modelo (LSTM - Clasificaci√≥n):")
print(f"  - Accuracy:  {accuracy:.4f}")
print(f"  - Precision: {precision:.4f}")
print(f"  - Recall:    {recall:.4f}")
print(f"  - F1 Score:  {f1:.4f}")
print(f"‚è±Ô∏è Tiempo total de entrenamiento + optimizaci√≥n: {end_time - start_time:.2f} segundos")

# Guardar resultados
tiempo_total = end_time - start_time
resultados.agregar_resultados_objetivo3('LSTM', accuracy, precision, recall, f1, tiempo_total)
resultados.agregar_hiperparametros_obj3('LSTM', params)

"""## **E3 - MODELO 2: GRU**"""

import time
start_time = time.time()

# ------------------------------------------
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, GRU, Dense, Flatten, Concatenate, Lambda
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import optuna

# Divisi√≥n de los datos
X = df[['Origen_int', 'Destino_int', 'Duracion_minutos', 'Directo_binario',
        'Dias_hasta_vuelo', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Rango_Horario_Consulta_int']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalizaci√≥n
scaler = StandardScaler()
X_train[['Duracion_minutos', 'Dias_hasta_vuelo']] = scaler.fit_transform(X_train[['Duracion_minutos', 'Dias_hasta_vuelo']])
X_val[['Duracion_minutos', 'Dias_hasta_vuelo']] = scaler.transform(X_val[['Duracion_minutos', 'Dias_hasta_vuelo']])

embedding_dim = 8

# ------------------------------------------
def objective(trial):
    gru_units = trial.suggest_int('gru_units', 32, 128)
    dense_units = trial.suggest_int('dense_units', 32, 128)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)

    # Entradas
    input_origen = Input(shape=(1,))
    input_destino = Input(shape=(1,))
    input_salida = Input(shape=(1,))
    input_llegada = Input(shape=(1,))
    input_continuas = Input(shape=(3,))

    embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
    embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
    embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
    embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

    flat_origen = Flatten()(embedding_origen)
    flat_destino = Flatten()(embedding_destino)
    flat_salida = Flatten()(embedding_salida)
    flat_llegada = Flatten()(embedding_llegada)

    x = Concatenate()([flat_origen, flat_destino, flat_salida, flat_llegada, input_continuas])
    x = Lambda(lambda x: tf.expand_dims(x, axis=1))(x)
    x = GRU(gru_units, return_sequences=False)(x)
    x = Dense(dense_units, activation='relu')(x)
    out = Dense(y.nunique(), activation='softmax')(x)

    model = Model(inputs=[input_origen, input_destino, input_salida, input_llegada, input_continuas], outputs=out)
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    model.fit([X_train['Origen_int'], X_train['Destino_int'], X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
               X_train[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values],
              y_train,
              validation_data=([X_val['Origen_int'], X_val['Destino_int'], X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
                                X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values], y_val),
              epochs=100, batch_size=32, verbose=0)

    preds = model.predict([X_val['Origen_int'], X_val['Destino_int'], X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
                           X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values])
    predicted_classes = np.argmax(preds, axis=1)
    return accuracy_score(y_val, predicted_classes)

# ------------------------------------------
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=10)

# Mostrar los mejores hiperpar√°metros
print("\nMejores hiperpar√°metros encontrados:")
print(study.best_params)
params = study.best_params

# Tiempo total
end_time = time.time()
tiempo_total = end_time - start_time
print(f"\n‚è±Ô∏è Tiempo de ejecuci√≥n total: {tiempo_total:.2f} segundos")

# ------------------------------------------
# Reconstrucci√≥n del mejor modelo para obtener m√©tricas finales
params = study.best_params
gru_units = params['gru_units']
dense_units = params['dense_units']
learning_rate = params['learning_rate']

# Definir arquitectura con mejores hiperpar√°metros
input_origen = Input(shape=(1,))
input_destino = Input(shape=(1,))
input_salida = Input(shape=(1,))
input_llegada = Input(shape=(1,))
input_continuas = Input(shape=(3,))

embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

flat_origen = Flatten()(embedding_origen)
flat_destino = Flatten()(embedding_destino)
flat_salida = Flatten()(embedding_salida)
flat_llegada = Flatten()(embedding_llegada)

x = Concatenate()([flat_origen, flat_destino, flat_salida, flat_llegada, input_continuas])
x = Lambda(lambda x: tf.expand_dims(x, axis=1))(x)
x = GRU(gru_units)(x)
x = Dense(dense_units, activation='relu')(x)
out = Dense(y.nunique(), activation='softmax')(x)

final_model = Model(inputs=[input_origen, input_destino, input_salida, input_llegada, input_continuas], outputs=out)
final_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

final_model.fit([X_train['Origen_int'], X_train['Destino_int'], X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
                 X_train[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values],
                y_train,
                epochs=100, batch_size=32, verbose=0)

# Predicci√≥n y m√©tricas finales
preds = final_model.predict([X_val['Origen_int'], X_val['Destino_int'], X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
                             X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values])
y_pred = np.argmax(preds, axis=1)

accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted')
recall = recall_score(y_val, y_pred, average='weighted')
f1 = f1_score(y_val, y_pred, average='weighted')

print("\nüìä M√©tricas del mejor modelo (GRU - Clasificaci√≥n):")
print(f"  - Accuracy:  {accuracy:.4f}")
print(f"  - Precision: {precision:.4f}")
print(f"  - Recall:    {recall:.4f}")
print(f"  - F1 Score:  {f1:.4f}")

# Guardar resultados
resultados.agregar_resultados_objetivo3('GRU', accuracy, precision, recall, f1, tiempo_total)
resultados.agregar_hiperparametros_obj3('GRU', params)

"""## **E3 - MODELO 3: TCN**"""

import time
start_time = time.time()

# --------------------------
import pandas as pd
import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Dense, Conv1D, BatchNormalization, Flatten, MaxPooling1D
from tensorflow.keras.layers import Lambda, Concatenate
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import optuna

# Preparaci√≥n de datos
X = df[['Origen_int', 'Destino_int', 'Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo',
        'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Rango_Horario_Consulta_int']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos', 'Dias_hasta_vuelo']] = scaler.fit_transform(X_train[['Duracion_minutos', 'Dias_hasta_vuelo']])
X_val[['Duracion_minutos', 'Dias_hasta_vuelo']] = scaler.transform(X_val[['Duracion_minutos', 'Dias_hasta_vuelo']])

embedding_dim = 8

# -------------------------------------
def objective(trial):
    filters = trial.suggest_categorical('filters', [32, 64, 128])
    dense_units = trial.suggest_int('dense_units', 32, 128)
    learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)

    # Entradas
    input_origen = Input(shape=(1,))
    input_destino = Input(shape=(1,))
    input_salida = Input(shape=(1,))
    input_llegada = Input(shape=(1,))
    input_continuas = Input(shape=(3,))

    embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
    embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
    embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
    embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

    flat_origen = Flatten()(embedding_origen)
    flat_destino = Flatten()(embedding_destino)
    flat_salida = Flatten()(embedding_salida)
    flat_llegada = Flatten()(embedding_llegada)

    x = Concatenate()([flat_origen, flat_destino, flat_salida, flat_llegada, input_continuas])
    x = Lambda(lambda t: tf.expand_dims(t, axis=1))(x)
    x = BatchNormalization()(x)
    x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=1)(x)
    x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=2)(x)
    x = MaxPooling1D(pool_size=1)(x)
    x = Flatten()(x)
    x = Dense(dense_units, activation='relu')(x)
    output = Dense(y.nunique(), activation='softmax')(x)

    model = Model(inputs=[input_origen, input_destino, input_salida, input_llegada, input_continuas], outputs=output)
    model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

    model.fit([X_train['Origen_int'], X_train['Destino_int'], X_train['Hora_Salida_Rango_int'],
               X_train['Hora_Llegada_Rango_int'], X_train[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values],
              y_train,
              validation_data=([X_val['Origen_int'], X_val['Destino_int'], X_val['Hora_Salida_Rango_int'],
                                X_val['Hora_Llegada_Rango_int'], X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values], y_val),
              epochs=100, batch_size=32, verbose=0)

    predictions = model.predict([X_val['Origen_int'], X_val['Destino_int'], X_val['Hora_Salida_Rango_int'],
                                 X_val['Hora_Llegada_Rango_int'], X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values])
    predicted_classes = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(y_val, predicted_classes)
    return accuracy

# -------------------------------------
study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=10)

print("\nResumen de todos los trials:")
df_trials = study.trials_dataframe().sort_values(by='value', ascending=False)
print(df_trials[['number', 'value', 'params_filters', 'params_dense_units', 'params_learning_rate']])

print("\nMejores hiperpar√°metros encontrados:")
print(study.best_params)
params = study.best_params

# Tiempo total
end_time = time.time()
tiempo_total = end_time - start_time
print(f"\n‚è±Ô∏è Tiempo de ejecuci√≥n: {tiempo_total:.2f} segundos")

# -------------------------------------
# Reconstrucci√≥n del mejor modelo
best_params = study.best_params
filters = best_params['filters']
dense_units = best_params['dense_units']
learning_rate = best_params['learning_rate']

# Entradas
input_origen = Input(shape=(1,))
input_destino = Input(shape=(1,))
input_salida = Input(shape=(1,))
input_llegada = Input(shape=(1,))
input_continuas = Input(shape=(3,))

embedding_origen = Embedding(input_dim=X['Origen_int'].nunique()+1, output_dim=embedding_dim)(input_origen)
embedding_destino = Embedding(input_dim=X['Destino_int'].nunique()+1, output_dim=embedding_dim)(input_destino)
embedding_salida = Embedding(input_dim=X['Hora_Salida_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_salida)
embedding_llegada = Embedding(input_dim=X['Hora_Llegada_Rango_int'].nunique()+1, output_dim=embedding_dim)(input_llegada)

flat_origen = Flatten()(embedding_origen)
flat_destino = Flatten()(embedding_destino)
flat_salida = Flatten()(embedding_salida)
flat_llegada = Flatten()(embedding_llegada)

x = Concatenate()([flat_origen, flat_destino, flat_salida, flat_llegada, input_continuas])
x = Lambda(lambda t: tf.expand_dims(t, axis=1))(x)
x = BatchNormalization()(x)
x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=1)(x)
x = Conv1D(filters, kernel_size=2, padding='causal', activation='relu', dilation_rate=2)(x)
x = MaxPooling1D(pool_size=1)(x)
x = Flatten()(x)
x = Dense(dense_units, activation='relu')(x)
output = Dense(y.nunique(), activation='softmax')(x)

final_model = Model(inputs=[input_origen, input_destino, input_salida, input_llegada, input_continuas], outputs=output)
final_model.compile(optimizer=Adam(learning_rate=learning_rate), loss='sparse_categorical_crossentropy', metrics=['accuracy'])

# Entrenamiento final
final_model.fit([X_train['Origen_int'], X_train['Destino_int'], X_train['Hora_Salida_Rango_int'],
                 X_train['Hora_Llegada_Rango_int'], X_train[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values],
                y_train, epochs=100, batch_size=32, verbose=0)

# Predicci√≥n y m√©tricas
predictions = final_model.predict([X_val['Origen_int'], X_val['Destino_int'], X_val['Hora_Salida_Rango_int'],
                                   X_val['Hora_Llegada_Rango_int'], X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values])
y_pred = np.argmax(predictions, axis=1)

accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted')
recall = recall_score(y_val, y_pred, average='weighted')
f1 = f1_score(y_val, y_pred, average='weighted')

print("\nüìä M√©tricas del mejor modelo (TCN - Clasificaci√≥n):")
print(f"  - Accuracy:  {accuracy:.4f}")
print(f"  - Precision: {precision:.4f}")
print(f"  - Recall:    {recall:.4f}")
print(f"  - F1 Score:  {f1:.4f}")
print(f"  - Tiempo:  {tiempo_total:.2f}")

# Guardar resultados
resultados.agregar_resultados_objetivo3('TCN', accuracy, precision, recall, f1, tiempo_total)
resultados.agregar_hiperparametros_obj3('TCN', params)

"""## **E3 - MODELO 4: TRANSFORMERS**"""

import time
start_time = time.time()

import optuna
import tensorflow as tf
from tensorflow.keras.layers import Input, Embedding, Dense, MultiHeadAttention, LayerNormalization, Dropout, Concatenate, Flatten, Lambda
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np

# --- Preparaci√≥n de datos ---
X = df[['Origen_int', 'Destino_int', 'Duracion_minutos', 'Directo_binario',
        'Dias_hasta_vuelo', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Rango_Horario_Consulta_int']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos', 'Dias_hasta_vuelo']] = scaler.fit_transform(X_train[['Duracion_minutos', 'Dias_hasta_vuelo']])
X_val[['Duracion_minutos', 'Dias_hasta_vuelo']] = scaler.transform(X_val[['Duracion_minutos', 'Dias_hasta_vuelo']])

# --- Funci√≥n objetivo para Optuna ---
def objective(trial):
    embedding_dim = trial.suggest_categorical("embedding_dim", [4, 8, 16])
    num_heads = trial.suggest_int("num_heads", 2, 4)
    ffn_units = trial.suggest_categorical("ffn_units", [32, 64, 128])
    dropout_rate = trial.suggest_float("dropout_rate", 0.1, 0.5)
    learning_rate = trial.suggest_float("learning_rate", 1e-4, 1e-2, log=True)

    # Entradas
    input_origen = Input(shape=(1,))
    input_destino = Input(shape=(1,))
    input_salida = Input(shape=(1,))
    input_llegada = Input(shape=(1,))
    input_continuas = Input(shape=(3,))

    embedding_origen = Embedding(X['Origen_int'].nunique()+1, embedding_dim)(input_origen)
    embedding_destino = Embedding(X['Destino_int'].nunique()+1, embedding_dim)(input_destino)
    embedding_salida = Embedding(X['Hora_Salida_Rango_int'].nunique()+1, embedding_dim)(input_salida)
    embedding_llegada = Embedding(X['Hora_Llegada_Rango_int'].nunique()+1, embedding_dim)(input_llegada)

    x = Concatenate()([embedding_origen, embedding_destino, embedding_salida, embedding_llegada])
    x = Flatten()(x)
    x = Concatenate()([x, input_continuas])
    x = Lambda(lambda x: tf.expand_dims(x, axis=1))(x)

    attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)
    attention = Dropout(dropout_rate)(attention)
    attention = LayerNormalization(epsilon=1e-6)(attention + x)

    ffn = Dense(x.shape[-1], activation='relu')(attention)
    ffn = Dense(x.shape[-1], activation='relu')(ffn)
    ffn = Dropout(dropout_rate)(ffn)
    ffn_output = LayerNormalization(epsilon=1e-6)(ffn + attention)

    x = Flatten()(ffn_output)
    x = Dense(ffn_units, activation='relu')(x)
    output = Dense(len(y.unique()), activation='softmax')(x)

    model = Model(inputs=[input_origen, input_destino, input_salida, input_llegada, input_continuas], outputs=output)
    model.compile(optimizer=Adam(learning_rate=learning_rate),
                  loss='sparse_categorical_crossentropy',
                  metrics=['accuracy'])

    model.fit(
        [X_train['Origen_int'], X_train['Destino_int'],
         X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
         X_train[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values],
        y_train,
        validation_data=(
            [X_val['Origen_int'], X_val['Destino_int'],
             X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
             X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values],
            y_val
        ),
        epochs=100,
        batch_size=32,
        verbose=0
    )

    y_pred_probs = model.predict([
        X_val['Origen_int'], X_val['Destino_int'],
        X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
        X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values
    ])
    y_pred = np.argmax(y_pred_probs, axis=1)
    acc = accuracy_score(y_val, y_pred)

    return 1.0 - acc  # Minimizar el error

# --- Optuna Search ---
print("üîç Optimizando Transformer con Optuna...")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)

best_params = study.best_params
print("\nüìå Mejores hiperpar√°metros encontrados:")
for k, v in best_params.items():
    print(f"  - {k}: {v}")

# --- Entrenamiento final con mejores hiperpar√°metros ---
embedding_dim = best_params['embedding_dim']
num_heads = best_params['num_heads']
ffn_units = best_params['ffn_units']
dropout_rate = best_params['dropout_rate']
learning_rate = best_params['learning_rate']

input_origen = Input(shape=(1,))
input_destino = Input(shape=(1,))
input_salida = Input(shape=(1,))
input_llegada = Input(shape=(1,))
input_continuas = Input(shape=(3,))

embedding_origen = Embedding(X['Origen_int'].nunique()+1, embedding_dim)(input_origen)
embedding_destino = Embedding(X['Destino_int'].nunique()+1, embedding_dim)(input_destino)
embedding_salida = Embedding(X['Hora_Salida_Rango_int'].nunique()+1, embedding_dim)(input_salida)
embedding_llegada = Embedding(X['Hora_Llegada_Rango_int'].nunique()+1, embedding_dim)(input_llegada)

x = Concatenate()([embedding_origen, embedding_destino, embedding_salida, embedding_llegada])
x = Flatten()(x)
x = Concatenate()([x, input_continuas])
x = Lambda(lambda x: tf.expand_dims(x, axis=1))(x)

attention = MultiHeadAttention(num_heads=num_heads, key_dim=embedding_dim)(x, x)
attention = Dropout(dropout_rate)(attention)
attention = LayerNormalization(epsilon=1e-6)(attention + x)

ffn = Dense(x.shape[-1], activation='relu')(attention)
ffn = Dense(x.shape[-1], activation='relu')(ffn)
ffn = Dropout(dropout_rate)(ffn)
ffn_output = LayerNormalization(epsilon=1e-6)(ffn + attention)

x = Flatten()(ffn_output)
x = Dense(ffn_units, activation='relu')(x)
output = Dense(len(y.unique()), activation='softmax')(x)

model_transformer = Model(inputs=[input_origen, input_destino, input_salida, input_llegada, input_continuas], outputs=output)
model_transformer.compile(optimizer=Adam(learning_rate=learning_rate),
                          loss='sparse_categorical_crossentropy',
                          metrics=['accuracy'])

model_transformer.fit(
    [X_train['Origen_int'], X_train['Destino_int'],
     X_train['Hora_Salida_Rango_int'], X_train['Hora_Llegada_Rango_int'],
     X_train[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values],
    y_train,
    validation_data=(
        [X_val['Origen_int'], X_val['Destino_int'],
         X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
         X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values],
        y_val
    ),
    epochs=100,
    batch_size=32,
    verbose=0
)

# --- Evaluaci√≥n final ---
end_time = time.time()
tiempo_total = end_time - start_time

y_pred_probs = model_transformer.predict([
    X_val['Origen_int'], X_val['Destino_int'],
    X_val['Hora_Salida_Rango_int'], X_val['Hora_Llegada_Rango_int'],
    X_val[['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']].values
])
y_pred = np.argmax(y_pred_probs, axis=1)

accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)

print("\n‚úÖ RESULTADOS TRANSFORMER OPTIMIZADO")
print(f"  - Accuracy:  {accuracy:.4f}")
print(f"  - Precision: {precision:.4f}")
print(f"  - Recall:    {recall:.4f}")
print(f"  - F1 Score:  {f1:.4f}")
print(f"  - Tiempo total: {tiempo_total:.2f} seg")

# --- Registro de resultados
resultados.agregar_resultados_objetivo3('TRANSFORMERS', accuracy, precision, recall, f1, tiempo_total)
resultados.agregar_hiperparametros_obj3('TRANSFORMERS', best_params)

"""## **E3 - MODELO 5: RANDOM FOREST**"""

import time
start_time = time.time()

# -------------------- LIBRER√çAS --------------------
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
import optuna

# -------------------- DATOS --------------------
X = df[['Origen_int', 'Destino_int', 'Duracion_minutos', 'Directo_binario',
        'Dias_hasta_vuelo', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Rango_Horario_Consulta_int']

X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# -------------------- COLUMNAS --------------------
cat_cols = ['Origen_int', 'Destino_int', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']
num_cols = ['Duracion_minutos', 'Dias_hasta_vuelo']

# -------------------- TRANSFORMADORES --------------------
preprocessor = ColumnTransformer(transformers=[
    ('num', StandardScaler(), num_cols),
    ('cat', OneHotEncoder(handle_unknown='ignore'), cat_cols)
])

# -------------------- OPTUNA --------------------
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 300),
        'max_depth': trial.suggest_int('max_depth', 100, 100),
        'min_samples_split': trial.suggest_int('min_samples_split', 2, 10),
        'min_samples_leaf': trial.suggest_int('min_samples_leaf', 1, 5),
        'random_state': 42,
        'n_jobs': -1
    }

    clf = Pipeline(steps=[
        ('preprocessor', preprocessor),
        ('classifier', RandomForestClassifier(**params))
    ])

    clf.fit(X_train, y_train)
    preds = clf.predict(X_val)
    acc = accuracy_score(y_val, preds)
    return acc

study = optuna.create_study(direction='maximize')
study.optimize(objective, n_trials=10)

# -------------------- RESULTADOS --------------------
print("\nüìå Mejores hiperpar√°metros encontrados:")
for key, val in study.best_params.items():
    print(f"  - {key}: {val}")
params = study.best_params
# Modelo final con mejores par√°metros
final_model = Pipeline(steps=[
    ('preprocessor', preprocessor),
    ('classifier', RandomForestClassifier(**study.best_params, random_state=42, n_jobs=-1))
])
final_model.fit(X_train, y_train)
final_preds = final_model.predict(X_val)

# M√©tricas
accuracy = accuracy_score(y_val, final_preds)
precision = precision_score(y_val, final_preds, average='weighted', zero_division=0)
recall = recall_score(y_val, final_preds, average='weighted', zero_division=0)
f1 = f1_score(y_val, final_preds, average='weighted', zero_division=0)

print("\nüìã Reporte de clasificaci√≥n:\n")
print(classification_report(y_val, final_preds, zero_division=0))

end_time = time.time()
tiempo_total = end_time - start_time
print(f"\n‚è±Ô∏è Tiempo total de ejecuci√≥n: {tiempo_total:.4f} segundos")

# -------------------- REGISTRO --------------------
resultados.agregar_resultados_objetivo3('Random Forest', accuracy, precision, recall, f1, tiempo_total)
resultados.agregar_hiperparametros_obj3('Random Forest', params)

"""## **E3 - MODELO 6: ARBOL DE DECISI√ìN**"""

import optuna
import time
import numpy as np
import pandas as pd
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from dirty_cat import SimilarityEncoder
from sklearn.compose import ColumnTransformer

# --- Tiempo total
start_time = time.time()

# --- Columnas
columnas_categoricas = ['Origen_int', 'Destino', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']
columnas_numericas = ['Duracion_minutos', 'Dias_hasta_vuelo']
X = df[columnas_categoricas + columnas_numericas]
y = df['Rango_Horario_Consulta_int']

# --- Split
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)

# --- Preprocesamiento con SimilarityEncoder + StandardScaler
encoder = SimilarityEncoder(similarity='ngram', ngram_range=(2, 4), categories='auto', random_state=42)
preprocessor = ColumnTransformer([
    ('cat', encoder, columnas_categoricas),
    ('num', StandardScaler(), columnas_numericas)
])

X_train_enc = preprocessor.fit_transform(X_train)
X_val_enc = preprocessor.transform(X_val)

# --- Funci√≥n objetivo Optuna
def objective(trial):
    max_depth = trial.suggest_int('max_depth', 100, 100)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 5)
    min_samples_leaf = trial.suggest_int('min_samples_leaf', 1, 5)
    max_features = trial.suggest_categorical('max_features', [None, 'sqrt', 'log2'])

    model = DecisionTreeClassifier(
        random_state=42,
        max_depth=max_depth,
        min_samples_split=min_samples_split,
        min_samples_leaf=min_samples_leaf,
        max_features=max_features
    )
    model.fit(X_train_enc, y_train)
    y_pred = model.predict(X_val_enc)
    return 1 - accuracy_score(y_val, y_pred)  # minimizar el error

# --- Optuna Study
print("‚è≥ Optimizando √Årbol de Decisi√≥n con Optuna (Objetivo 3)...")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)

# --- Hiperpar√°metros √≥ptimos
best_params = study.best_params
print("\nüìå Mejores hiperpar√°metros √Årbol de Decisi√≥n (Embeddings):")
for k, v in best_params.items():
    print(f"  - {k}: {v}")

# --- Entrenamiento final
print("\nüìà Entrenando √Årbol de Decisi√≥n con embeddings...")
model = DecisionTreeClassifier(random_state=42, **best_params)
model.fit(X_train_enc, y_train)

# --- Evaluaci√≥n
print("\nüìä Evaluando √Årbol de Decisi√≥n con embeddings...")
y_pred = model.predict(X_val_enc)
accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)

print("\nüìã Reporte de clasificaci√≥n:\n")
print(classification_report(y_val, y_pred, zero_division=0))

tiempo_total = time.time() - start_time
print(f"‚è±Ô∏è Tiempo de ejecuci√≥n: {tiempo_total:.2f} segundos")

# --- Guardar resultados
resultados.agregar_resultados_objetivo3('Arbol de Decision', accuracy, precision, recall, f1, tiempo_total)
resultados.agregar_hiperparametros_obj3('Arbol de Decision', best_params)

"""## **E3 - MODELO 7: KNN**"""

import time
import optuna
import numpy as np
import pandas as pd
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate
import tensorflow as tf

# --- Datos
categorical_features = ['Origen_int', 'Destino_int', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']
numerical_features = ['Duracion_minutos', 'Directo_binario', 'Dias_hasta_vuelo']
target = 'Rango_Horario_Consulta_int'

X_cat = df[categorical_features].copy()
X_num = df[numerical_features].copy()
y = df[target].copy()

# --- Normalizaci√≥n
scaler = StandardScaler()
X_num[numerical_features] = scaler.fit_transform(X_num[numerical_features])

# --- Embedding model
inputs = []
embeddings = []

for col in categorical_features:
    input_cat = Input(shape=(1,), name=f'{col}_input')
    vocab_size = df[col].nunique()
    embed_dim = min(50, vocab_size // 2 + 1)
    embed = Embedding(input_dim=vocab_size + 1, output_dim=embed_dim, name=f'{col}_embedding')(input_cat)
    embed = Flatten()(embed)
    inputs.append(input_cat)
    embeddings.append(embed)

input_num = Input(shape=(X_num.shape[1],), name='numerical_input')
inputs.append(input_num)
x = Concatenate()(embeddings + [input_num])

embedding_model = Model(inputs=inputs, outputs=x)

# --- Preparar X_embed
X_inputs = [X_cat[col].values for col in categorical_features] + [X_num.values]
X_embed = embedding_model.predict(X_inputs, verbose=0)

# --- Divisi√≥n
X_train, X_val, y_train, y_val = train_test_split(X_embed, y, test_size=0.2, stratify=y, random_state=42)

# --- Optuna
start_total = time.time()

def objective(trial):
    n_neighbors = trial.suggest_int('n_neighbors', 100, 100)
    weights = trial.suggest_categorical('weights', ['uniform', 'distance'])
    p = trial.suggest_int('p', 1, 2)

    model = KNeighborsClassifier(n_neighbors=n_neighbors, weights=weights, p=p)
    model.fit(X_train, y_train)
    y_pred = model.predict(X_val)
    return 1.0 - f1_score(y_val, y_pred, average='weighted', zero_division=0)

print("‚è≥ Optimizando KNN con Optuna (clasificaci√≥n)...")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10, n_jobs=1)

best_params = study.best_params
print("\nüìå Mejores hiperpar√°metros KNN:")
for k, v in best_params.items():
    print(f"  - {k}: {v}")

# --- Entrenamiento final
print("\nüìà Entrenando mejor modelo KNN...")
start_train = time.time()
knn_final = KNeighborsClassifier(**best_params)
knn_final.fit(X_train, y_train)
train_time = time.time() - start_train

# --- Evaluaci√≥n
print("\nüìä Evaluando modelo final...")
y_pred_final = knn_final.predict(X_val)

accuracy = accuracy_score(y_val, y_pred_final)
precision = precision_score(y_val, y_pred_final, average='weighted', zero_division=0)
recall = recall_score(y_val, y_pred_final, average='weighted', zero_division=0)
f1 = f1_score(y_val, y_pred_final, average='weighted', zero_division=0)
total_time = time.time() - start_total

print(f"\n‚úÖ KNN Clasificaci√≥n - M√©tricas:")
print(f"  - Accuracy:  {accuracy:.4f}")
print(f"  - Precision: {precision:.4f}")
print(f"  - Recall:    {recall:.4f}")
print(f"  - F1-score:  {f1:.4f}")
print(f"‚è±Ô∏è Tiempo total de ejecuci√≥n: {total_time:.2f} seg")
print("\nüìã Reporte de clasificaci√≥n completo:")
print(classification_report(y_val, y_pred_final, zero_division=0))

# --- Guardar resultados
resultados.agregar_resultados_objetivo3('KNN', accuracy, precision, recall, f1, total_time)
resultados.agregar_hiperparametros_obj3('KNN', best_params)

"""## **E3 - MODELO 8: XGBoost**"""

import time
import optuna
import numpy as np
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# --- Selecci√≥n de columnas (con embeddings u otras)
columnas_embeddings = [col for col in df.columns if col.startswith('Origen_emb_') or col.startswith('Destino_emb_')]
X = df[columnas_embeddings + ['Duracion_minutos', 'Directo_binario', 'Hora_Salida_Rango_int', 'Hora_Llegada_Rango_int']]
y = df['Rango_Horario_Consulta_int']

# --- Divisi√≥n y escalado
X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)

scaler = StandardScaler()
X_train[['Duracion_minutos']] = scaler.fit_transform(X_train[['Duracion_minutos']])
X_val[['Duracion_minutos']] = scaler.transform(X_val[['Duracion_minutos']])

# --- Optuna
def objective(trial):
    params = {
        'n_estimators': trial.suggest_int('n_estimators', 100, 200),
        'max_depth': trial.suggest_int('max_depth', 100, 100),
        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'gamma': trial.suggest_float('gamma', 0, 5),
        'random_state': 42,
        'use_label_encoder': False,
        'eval_metric': 'mlogloss',
    }

    model = XGBClassifier(**params)
    model.fit(X_train, y_train)
    preds = model.predict(X_val)
    f1 = f1_score(y_val, preds, average='weighted', zero_division=0)
    return 1.0 - f1  # Minimizar el error

# --- Tiempo total
start_total = time.time()

print("‚è≥ Optimizando XGBoost con Optuna...")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=10)

opt_time = time.time() - start_total

# --- Mejor modelo
best_params = study.best_params
best_params.update({
    'random_state': 42,
    'use_label_encoder': False,
    'eval_metric': 'mlogloss'
})

print("\nüìå Mejores hiperpar√°metros XGBoost:")
for k, v in best_params.items():
    print(f"  - {k}: {v}")
print(f"\n‚è±Ô∏è Tiempo total (Optuna + entrenamiento): {opt_time:.2f} segundos")

# --- Entrenamiento final
print("\nüìà Entrenando mejor modelo XGBoost...")
model_xgb = XGBClassifier(**best_params)
model_xgb.fit(X_train, y_train)

# --- Evaluaci√≥n
print("\nüìä Evaluando modelo final...")
y_pred = model_xgb.predict(X_val)

accuracy = accuracy_score(y_val, y_pred)
precision = precision_score(y_val, y_pred, average='weighted', zero_division=0)
recall = recall_score(y_val, y_pred, average='weighted', zero_division=0)
f1 = f1_score(y_val, y_pred, average='weighted', zero_division=0)

print(f"\n‚úÖ XGBoost Metrics:")
print(f"  - Accuracy:  {accuracy:.4f}")
print(f"  - Precision: {precision:.4f}")
print(f"  - Recall:    {recall:.4f}")
print(f"  - F1-score:  {f1:.4f}")

print("\nüìã Reporte de clasificaci√≥n completo:")
print(classification_report(y_val, y_pred, zero_division=0))

# --- Guardar resultados
resultados.agregar_resultados_objetivo3('XGBoost', accuracy, precision, recall, f1, opt_time)
resultados.agregar_hiperparametros_obj3('XGBoost', best_params)

"""#RESULTADOS SCRIPT"""

resultados.guardar_csv('resultados_completos.csv')
resultados.mostrar_resultados()